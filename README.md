Here is the final, professionally formatted **`README.md`** file. It is written in Markdown, which GitHub will automatically turn into a beautiful landing page.

### ğŸ“ Final `README.md`

Copy everything below and paste it into the **`README.md`** file in your GitHub repository:

```markdown
# TMMLU+ Leaderboard ğŸ‡¹ğŸ‡¼

[![GitHub Pages](https://img.shields.io/badge/GitHub%20Pages-Live-brightgreen)](https://muhammadsaqlainaslam.github.io/tmmlu-leaderboard/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ“Œ Project Overview
**TMMLU+** (Traditional Chinese Massive Multitask Language Understanding) is a state-of-the-art benchmark designed to evaluate Large Language Models (LLMs) specifically within the linguistic and cultural context of **Taiwan**.

The benchmark includes **66 subjects** covering STEM, Social Sciences, Humanities, and professional certifications (e.g., Real Estate, Clinical Psychology, and Law). This leaderboard provides an interactive platform to compare model performance across these diverse domains.

## ğŸ“Š Live Interactive Leaderboard
We provide an interactive dashboard that offers more than just raw scores:
- **Search & Filter:** Find specific models instantly.
- **Visual Analytics:** Compare performance via Discipline Radar Maps and Category Bar Charts.
- **Nested Drill-down:** Click on a model to see Major Disciplines, and expand those to see scores for all 66+ individual subjects.
- **General Benchmarks:** Includes external evaluations like DRCD, TW-RAG, and GSM8K.

ğŸ‘‰ **[Access the Interactive Leaderboard Here](https://muhammadsaqlainaslam.github.io/tmmlu-leaderboard/)**

## ğŸ“‚ Repository Structure
```text
â”œâ”€â”€ .github/ISSUE_TEMPLATE/  # Submission form configuration
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ index.html           # Website Frontend (Plotly, PapaParse, Bootstrap)
â”œâ”€â”€ results/
â”‚   â””â”€â”€ benchmark.csv        # Central Data Source (Update this to add models)
â””â”€â”€ README.md                # Project Documentation

```

## ğŸš€ How to Submit Results

We welcome contributions from researchers and developers. To add your model to the leaderboard:

1. **Prepare Data:** Ensure your results are calculated using the TMMLU+ methodology and formatted to match `results/benchmark.csv`.
2. **Submit an Issue:** Click the **"Submit Your Model Results"** button on the live leaderboard website to fill out a pre-formatted request.
3. **Pull Request:** Alternatively, fork this repo, add your model's column to the CSV, and submit a PR.

## ğŸ“„ Citation

If you use this benchmark or leaderboard in your research, please cite it as follows:

```bibtex
@misc{aslam2025tmmluplus,
  author = {Aslam, Muhammad Saqlain},
  title = {TMMLU+ Leaderboard: Traditional Chinese Massive Multitask Language Understanding Benchmark},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub Repository},
  howpublished = {\url{[https://github.com/MuhammadSaqlainAslam/tmmlu-leaderboard](https://github.com/MuhammadSaqlainAslam/tmmlu-leaderboard)}}
}

```

---

**Maintained by:** [Muhammad Saqlain Aslam](https://www.google.com/search?q=https://github.com/MuhammadSaqlainAslam)

*Built for the Traditional Chinese NLP Community.*

```

---

### One Final Step for the Submission Button
As mentioned before, for that blue "Submit Your Model Results" button to work, you must create the template file. 



1. Create a folder named `.github` (at the same level as `docs`).
2. Inside it, create a folder named `ISSUE_TEMPLATE`.
3. Inside that, create a file named `model_submission.md` and paste the template content I provided in our previous message.

**Your project is now officially complete and ready for public sharing! Would you like me to help you draft a LinkedIn or Twitter post to announce it?**

```
